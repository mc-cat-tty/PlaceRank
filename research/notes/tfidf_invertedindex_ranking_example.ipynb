{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import string\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load 10k reuters documents\n",
    "len(reuters.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\\n  Mounting trade friction between the\\n  U.S. And Japan has raised fears among many of Asia's exporting\\n  nations that the row could inflict far-reaching\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view text from one doc\n",
    "reuters.raw(fileids=['test/14826'])[0:201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHINA DAILY SAYS VERMIN EAT 712 PCT GRAIN STOCKS\n",
      "  A survey of 19 provinces and seven cities\n",
      "  showed vermin consume between seven and 12 pct of Chinas grain\n",
      "  stocks the China Daily said\n",
      "      It also said that each year 1575 mln tonnes or 25 pct of\n",
      "  Chinas fruit output are left to rot and 21 mln tonnes or up\n",
      "  to 30 pct of its vegetables The paper blamed the waste on\n",
      "  inadequate storage and bad preservation methods\n",
      "      It said the government had launched a national programme to\n",
      "  reduce waste calling for improved technology in storage and\n",
      "  preservation and greater production of additives The paper\n",
      "  gave no further details\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exclude = set(string.punctuation)\n",
    "alldocslist = []\n",
    "\n",
    "for index, i in  enumerate(reuters.fileids()):\n",
    "    text = reuters.raw(fileids=[i])\n",
    "    text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    alldocslist.append(text)\n",
    "    \n",
    "print(alldocslist[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHINA',\n",
       " 'DAILY',\n",
       " 'SAYS',\n",
       " 'VERMIN',\n",
       " 'EAT',\n",
       " '712',\n",
       " 'PCT',\n",
       " 'GRAIN',\n",
       " 'STOCKS',\n",
       " 'A',\n",
       " 'survey',\n",
       " 'of',\n",
       " '19',\n",
       " 'provinces',\n",
       " 'and',\n",
       " 'seven',\n",
       " 'cities',\n",
       " 'showed',\n",
       " 'vermin',\n",
       " 'consume',\n",
       " 'between',\n",
       " 'seven',\n",
       " 'and',\n",
       " '12',\n",
       " 'pct',\n",
       " 'of',\n",
       " 'Chinas',\n",
       " 'grain',\n",
       " 'stocks',\n",
       " 'the',\n",
       " 'China',\n",
       " 'Daily',\n",
       " 'said',\n",
       " 'It',\n",
       " 'also',\n",
       " 'said',\n",
       " 'that',\n",
       " 'each',\n",
       " 'year',\n",
       " '1575',\n",
       " 'mln',\n",
       " 'tonnes',\n",
       " 'or',\n",
       " '25',\n",
       " 'pct',\n",
       " 'of',\n",
       " 'Chinas',\n",
       " 'fruit',\n",
       " 'output',\n",
       " 'are',\n",
       " 'left',\n",
       " 'to',\n",
       " 'rot',\n",
       " 'and',\n",
       " '21',\n",
       " 'mln',\n",
       " 'tonnes',\n",
       " 'or',\n",
       " 'up',\n",
       " 'to',\n",
       " '30',\n",
       " 'pct',\n",
       " 'of',\n",
       " 'its',\n",
       " 'vegetables',\n",
       " 'The',\n",
       " 'paper',\n",
       " 'blamed',\n",
       " 'the',\n",
       " 'waste',\n",
       " 'on',\n",
       " 'inadequate',\n",
       " 'storage',\n",
       " 'and',\n",
       " 'bad',\n",
       " 'preservation',\n",
       " 'methods',\n",
       " 'It',\n",
       " 'said',\n",
       " 'the',\n",
       " 'government',\n",
       " 'had',\n",
       " 'launched',\n",
       " 'a',\n",
       " 'national',\n",
       " 'programme',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'waste',\n",
       " 'calling',\n",
       " 'for',\n",
       " 'improved',\n",
       " 'technology',\n",
       " 'in',\n",
       " 'storage',\n",
       " 'and',\n",
       " 'preservation',\n",
       " 'and',\n",
       " 'greater',\n",
       " 'production',\n",
       " 'of',\n",
       " 'additives',\n",
       " 'The',\n",
       " 'paper',\n",
       " 'gave',\n",
       " 'no',\n",
       " 'further',\n",
       " 'details']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize words\n",
    "plot_data = [[]] * len(alldocslist)\n",
    "for doc in alldocslist:\n",
    "    text = doc\n",
    "    tokentext = word_tokenize(text)\n",
    "    plot_data[index].append(tokentext)\n",
    "\n",
    "(plot_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHINA',\n",
       " 'DAILY',\n",
       " 'SAYS',\n",
       " 'VERMIN',\n",
       " 'EAT',\n",
       " '712',\n",
       " 'PCT',\n",
       " 'GRAIN',\n",
       " 'STOCKS',\n",
       " 'A']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "plot_data[0][1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['china',\n",
       " 'daily',\n",
       " 'says',\n",
       " 'vermin',\n",
       " 'eat',\n",
       " '712',\n",
       " 'pct',\n",
       " 'grain',\n",
       " 'stocks',\n",
       " 'a']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all words lowercase\n",
    "for x in range (len(reuters.fileids())):\n",
    "    lowers = [word.lower() for word in plot_data[0][x]]\n",
    "    plot_data[0][x] = lowers\n",
    "\n",
    "plot_data[0][1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['china',\n",
       " 'daily',\n",
       " 'says',\n",
       " 'vermin',\n",
       " 'eat',\n",
       " '712',\n",
       " 'pct',\n",
       " 'grain',\n",
       " 'stocks',\n",
       " 'survey']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "for x in range(len(reuters.fileids())):\n",
    "    filtered_sentence = [w for w in plot_data[0][x] if not w in stop_words]\n",
    "    plot_data[0][x] = filtered_sentence\n",
    "\n",
    "plot_data[0][1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ltaha',\n",
       " 'automot',\n",
       " 'technolog',\n",
       " 'corp',\n",
       " 'year',\n",
       " 'net',\n",
       " 'shr',\n",
       " '43',\n",
       " 'ct',\n",
       " 'vs']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stemming\n",
    "my_stemmer = SnowballStemmer(\"english\")\n",
    "stemmed_sentence = [my_stemmer.stem(w) for w in filtered_sentence]\n",
    "stemmed_sentence[0:10]\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "my_stemmer = SnowballStemmer(\"english\")\n",
    "stemmed_sentence = [porter_stemmer.stem(w) for w in filtered_sentence]\n",
    "stemmed_sentence[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = plot_data[0]\n",
    "flatten = [item for sublist in l for item in sublist]\n",
    "words = flatten\n",
    "words_unique = set(words)\n",
    "words_unique = list(words_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "\n",
    "def n_containing(word, doclist):\n",
    "    return sum(1 for doc in doclist if word in doc)\n",
    "\n",
    "def idf(word, doclist):\n",
    "    return math.log(len(doclist) / (0.01 + n_containing(word, doclist)))\n",
    "\n",
    "def tfidf(word, doc, doclist):\n",
    "    return (tf(word, doc) * idf(word, doclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of words\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "plottest = plot_data[0][0:1000]\n",
    "\n",
    "worddic = {}\n",
    "\n",
    "for doc in plottest:\n",
    "    for word in words_unique:\n",
    "        if word in doc:\n",
    "            word = str(word)\n",
    "            index = plottest.index(doc)\n",
    "            positions = list(np.where(np.array(plottest[index]) == word)[0])\n",
    "            idfs = tfidf(word, doc, plottest)\n",
    "            try:\n",
    "                worddic[word].append([index, positions, idfs])\n",
    "            except:\n",
    "                worddic[word] = []\n",
    "                worddic[word].append([index, positions, idfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, [0, 23], 0.1131500878815288],\n",
       " [13, [0], 0.06694713532990454],\n",
       " [14, [160], 0.013213250394060107],\n",
       " [28, [51], 0.05821490028687352],\n",
       " [40, [3, 15, 59, 79], 0.14740653650621185],\n",
       " [236, [86], 0.04414096834938761],\n",
       " [281, [70], 0.0565750439407644],\n",
       " [293, [13, 21], 0.11642980057374704],\n",
       " [302, [33], 0.059952658504392124],\n",
       " [342, [55, 146], 0.05391715597039292],\n",
       " [567, [2], 0.06925565723783228],\n",
       " [569, [1014, 1072, 1221], 0.009248261212112677],\n",
       " [612, [20], 0.01998421950146404],\n",
       " [710, [0, 7, 34], 0.17464470086062053],\n",
       " [720, [0, 16], 0.23628400704672192],\n",
       " [721, [0, 6, 27, 78, 82], 0.2028701070603168],\n",
       " [733, [179], 0.021595850106420823],\n",
       " [736, [0, 5, 21, 83], 0.13732745708698368]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the index create a dic where each word is a KEY and a list\n",
    "# of doc indexes, word positions, and td-idf score as VALUES\n",
    "\n",
    "worddic['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dic\n",
    "np.save('worddic_1000.npy', worddic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['indonesia', 'crude', 'palm', 'oil']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def search(searchsentence):\n",
    "    try:\n",
    "        searchsentence = searchsentence.lower()\n",
    "        try:\n",
    "            words = searchsentence.split(' ')\n",
    "        except:\n",
    "            words = list(words)\n",
    "        enddic = {}\n",
    "        idfdic = {}\n",
    "        closedic = {}\n",
    "\n",
    "        realwords = []\n",
    "        for word in words:\n",
    "            if word in list(worddic.keys()):\n",
    "                realwords.append(word)\n",
    "        words = realwords\n",
    "        numwords = len(words)\n",
    "\n",
    "        for word in words:\n",
    "            for indpos in worddic[word]:\n",
    "                index = indpos[0]\n",
    "                amount = len(indpos[1])\n",
    "                idfscore = indpos[2]\n",
    "                enddic[index] = amount\n",
    "                idfdic[index] = idfscore\n",
    "                fullcount_order = sorted(enddic.items(), key = lambda x:x[1], reverse=True)\n",
    "                fullidf_order = sorted(idfdic.items(), key = lambda x:x[1], reverse=True)\n",
    "\n",
    "        combo = []\n",
    "        alloptions = {k: worddic.get(k, None) for k in (words)}\n",
    "        for worddex in list(alloptions.values()):\n",
    "            for indexpos in worddex:\n",
    "                for indexz in indexpos:\n",
    "                    combo.append(indexz)\n",
    "        comboindex = combo[::3]\n",
    "        combocount = Counter(comboindex)\n",
    "        for key in combocount:\n",
    "            combocount[key] = combocount[key] / numwords\n",
    "        combocount_order = sorted(combocount.items(), key = lambda x:x[1], reverse=True)\n",
    "\n",
    "        if len(words) > 1:\n",
    "            x = []\n",
    "            y = []\n",
    "            for record in [worddic[z] for z in words]:\n",
    "                for index in record:\n",
    "                    x.append(index[0])\n",
    "            for i in x:\n",
    "                if x.count(i) > 1:\n",
    "                    y.append(i)\n",
    "            y = list(set(y))\n",
    "\n",
    "            closedic = {}\n",
    "            for wordbig in [worddic[x] for x in words]:\n",
    "                for record in wordbig:\n",
    "                    if record[0] in y:\n",
    "                        index = record[0]\n",
    "                        positions = record[1]\n",
    "                        try:\n",
    "                            closedic[index].append(positions)\n",
    "                        except:\n",
    "                            closedic[index] = []\n",
    "                            closedic[index].append(positions)\n",
    "            \n",
    "            x = 0\n",
    "            fdic = {}\n",
    "            for index in y:\n",
    "                csum = []\n",
    "                for seqlist in closedic[index]:\n",
    "                    while x > 0:\n",
    "                        secondlist = seqlist\n",
    "                        x = 0\n",
    "                        sol = [1 for i in firstlist if i + 1 in secondlist]\n",
    "                        csum.append(sol)\n",
    "                        fsum = [item for sublist in csum for item in sublist]\n",
    "                        fsum = sum(fsum)\n",
    "                        fdic[index] = fsum\n",
    "                        fdic_order = sorted(fdic.items(), key = lambda x:x[1], reverse=True)\n",
    "                    while x == 0:\n",
    "                        firstlist = seqlist\n",
    "                        x = x + 1\n",
    "        else:\n",
    "            fdic_order = 0\n",
    "        \n",
    "\n",
    "        return(searchsentence, words, fullcount_order, combocount_order, fullidf_order, fdic_order)\n",
    "    \n",
    "    except:\n",
    "        return(\"\")\n",
    "    \n",
    "\n",
    "#test search\n",
    "\n",
    "search('indonesia crude palm oil')[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crude', 'palm', 'oil']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('indonesia crude palm oil')[1][1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search term</th>\n",
       "      <th>actual_words_searched</th>\n",
       "      <th>num_occur</th>\n",
       "      <th>percentage_of_terms</th>\n",
       "      <th>td-idf</th>\n",
       "      <th>word_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>china daily says what</td>\n",
       "      <td>[china, daily, says]</td>\n",
       "      <td>[(183, 5), (40, 4), (569, 3), (710, 3), (342, ...</td>\n",
       "      <td>[(1, 1.0), (13, 0.6666666666666666), (14, 0.66...</td>\n",
       "      <td>[(675, 0.5095658223243495), (135, 0.4367707048...</td>\n",
       "      <td>[(1, 3), (293, 1), (720, 1), (721, 1), (736, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>indonesia crude palm oil</td>\n",
       "      <td>[indonesia, crude, palm, oil]</td>\n",
       "      <td>[(33, 13), (621, 12), (34, 11), (209, 8), (123...</td>\n",
       "      <td>[(4, 1.0), (6, 1.0), (209, 0.5), (281, 0.5), (...</td>\n",
       "      <td>[(762, 0.48707909813666866), (266, 0.434203698...</td>\n",
       "      <td>[(34, 6), (4, 5), (660, 5), (6, 4), (268, 2), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>price of nickel</td>\n",
       "      <td>[price, nickel]</td>\n",
       "      <td>[(572, 19), (639, 8), (108, 7), (148, 7), (736...</td>\n",
       "      <td>[(724, 1.0), (4, 0.5), (7, 0.5), (20, 0.5), (2...</td>\n",
       "      <td>[(50, 0.24460301234499893), (537, 0.2066299280...</td>\n",
       "      <td>[(724, 0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>north yemen sugar</td>\n",
       "      <td>[north, yemen, sugar]</td>\n",
       "      <td>[(700, 12), (96, 8), (494, 7), (296, 6), (525,...</td>\n",
       "      <td>[(30, 1.0), (758, 1.0), (47, 0.666666666666666...</td>\n",
       "      <td>[(494, 0.3808351739278394), (30, 0.35115970582...</td>\n",
       "      <td>[(758, 2), (30, 2), (851, 0), (47, 0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nippon steel</td>\n",
       "      <td>[nippon, steel]</td>\n",
       "      <td>[(40, 9), (253, 8), (444, 7), (223, 2), (435, ...</td>\n",
       "      <td>[(40, 1.0), (123, 0.5), (223, 0.5), (253, 0.5)...</td>\n",
       "      <td>[(223, 0.5682589478261134), (40, 0.42228417223...</td>\n",
       "      <td>[(40, 5)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>china</td>\n",
       "      <td>[china]</td>\n",
       "      <td>[(721, 5), (40, 4), (736, 4), (569, 3), (710, ...</td>\n",
       "      <td>[(1, 1.0), (13, 1.0), (14, 1.0), (28, 1.0), (4...</td>\n",
       "      <td>[(720, 0.23628400704672192), (721, 0.202870107...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gold</td>\n",
       "      <td>[gold]</td>\n",
       "      <td>[(997, 6), (20, 5), (797, 5), (341, 4), (347, ...</td>\n",
       "      <td>[(8, 1.0), (12, 1.0), (20, 1.0), (32, 1.0), (2...</td>\n",
       "      <td>[(304, 0.30902054113001826), (20, 0.2575171176...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>trade</td>\n",
       "      <td>[trade]</td>\n",
       "      <td>[(0, 15), (169, 10), (544, 10), (761, 8), (273...</td>\n",
       "      <td>[(285, 2.0), (701, 2.0), (713, 2.0), (923, 2.0...</td>\n",
       "      <td>[(223, 0.24728127372797265), (449, 0.247281273...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                search term          actual_words_searched  \\\n",
       "0     china daily says what           [china, daily, says]   \n",
       "1  indonesia crude palm oil  [indonesia, crude, palm, oil]   \n",
       "2           price of nickel                [price, nickel]   \n",
       "3         north yemen sugar          [north, yemen, sugar]   \n",
       "4              nippon steel                [nippon, steel]   \n",
       "5                     china                        [china]   \n",
       "6                      gold                         [gold]   \n",
       "7                     trade                        [trade]   \n",
       "\n",
       "                                           num_occur  \\\n",
       "0  [(183, 5), (40, 4), (569, 3), (710, 3), (342, ...   \n",
       "1  [(33, 13), (621, 12), (34, 11), (209, 8), (123...   \n",
       "2  [(572, 19), (639, 8), (108, 7), (148, 7), (736...   \n",
       "3  [(700, 12), (96, 8), (494, 7), (296, 6), (525,...   \n",
       "4  [(40, 9), (253, 8), (444, 7), (223, 2), (435, ...   \n",
       "5  [(721, 5), (40, 4), (736, 4), (569, 3), (710, ...   \n",
       "6  [(997, 6), (20, 5), (797, 5), (341, 4), (347, ...   \n",
       "7  [(0, 15), (169, 10), (544, 10), (761, 8), (273...   \n",
       "\n",
       "                                 percentage_of_terms  \\\n",
       "0  [(1, 1.0), (13, 0.6666666666666666), (14, 0.66...   \n",
       "1  [(4, 1.0), (6, 1.0), (209, 0.5), (281, 0.5), (...   \n",
       "2  [(724, 1.0), (4, 0.5), (7, 0.5), (20, 0.5), (2...   \n",
       "3  [(30, 1.0), (758, 1.0), (47, 0.666666666666666...   \n",
       "4  [(40, 1.0), (123, 0.5), (223, 0.5), (253, 0.5)...   \n",
       "5  [(1, 1.0), (13, 1.0), (14, 1.0), (28, 1.0), (4...   \n",
       "6  [(8, 1.0), (12, 1.0), (20, 1.0), (32, 1.0), (2...   \n",
       "7  [(285, 2.0), (701, 2.0), (713, 2.0), (923, 2.0...   \n",
       "\n",
       "                                              td-idf  \\\n",
       "0  [(675, 0.5095658223243495), (135, 0.4367707048...   \n",
       "1  [(762, 0.48707909813666866), (266, 0.434203698...   \n",
       "2  [(50, 0.24460301234499893), (537, 0.2066299280...   \n",
       "3  [(494, 0.3808351739278394), (30, 0.35115970582...   \n",
       "4  [(223, 0.5682589478261134), (40, 0.42228417223...   \n",
       "5  [(720, 0.23628400704672192), (721, 0.202870107...   \n",
       "6  [(304, 0.30902054113001826), (20, 0.2575171176...   \n",
       "7  [(223, 0.24728127372797265), (449, 0.247281273...   \n",
       "\n",
       "                                          word_order  \n",
       "0  [(1, 3), (293, 1), (720, 1), (721, 1), (736, 0...  \n",
       "1  [(34, 6), (4, 5), (660, 5), (6, 4), (268, 2), ...  \n",
       "2                                         [(724, 0)]  \n",
       "3             [(758, 2), (30, 2), (851, 0), (47, 0)]  \n",
       "4                                          [(40, 5)]  \n",
       "5                                                  0  \n",
       "6                                                  0  \n",
       "7                                                  0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save metrics to dataframe for use in ranking and machine learning \n",
    "result1 = search('china daily says what')\n",
    "result2 = search('indonesia crude palm oil')\n",
    "result3 = search('price of nickel')\n",
    "result4 = search('north yemen sugar')\n",
    "result5 = search('nippon steel')\n",
    "result6 = search('China')\n",
    "result7 = search('Gold')\n",
    "result8 = search('trade')\n",
    "df = pd.DataFrame([result1,result2,result3,result4,result5,result6,result7,result8])\n",
    "df.columns = ['search term', 'actual_words_searched','num_occur','percentage_of_terms','td-idf','word_order']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHINA DAILY SAYS VERMIN EAT 712 PCT GRAIN STOCKS\n",
      "  A survey of 19 provinces and seven cities\n",
      "  showed vermin consume between seven and 12 pct of Chinas grain\n",
      "  stocks the China Daily said\n",
      "      It also said that each year 1575 mln tonnes or 25 pct of\n",
      "  Chinas fruit output are left to rot and 21 mln tonnes or up\n",
      "  to 30 pct of its vegetables The paper blamed the waste on\n",
      "  inadequate storage and bad preservation methods\n",
      "      It said the government had launched a national programme to\n",
      "  reduce waste calling for improved technology in storage and\n",
      "  preservation and greater production of additives The paper\n",
      "  gave no further details\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(alldocslist[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(term):\n",
    "    results = search(term)\n",
    "\n",
    "    num_score = results[2]\n",
    "    per_score = results[3]\n",
    "    tfscore = results[4]\n",
    "    order_score = results[5]\n",
    "\n",
    "    final_candidates = []\n",
    "\n",
    "    try:\n",
    "        first_candidates = []\n",
    "\n",
    "        for candidates in order_score:\n",
    "            if candidates[1] > 1:\n",
    "                first_candidates.append(candidates[0])\n",
    "        \n",
    "        second_candidates = []\n",
    "\n",
    "        for match_candidates in per_score:\n",
    "            if match_candidates[1] == 1:\n",
    "                second_candidates.append(match_candidates[0])\n",
    "            if match_candidates[1] == 1 and match_candidates[0] in first_candidates:\n",
    "                final_candidates.append(match_candidates[0])\n",
    "        \n",
    "        t3_order = first_candidates[0:3]\n",
    "        for each in t3_order:\n",
    "            if each not in final_candidates:\n",
    "                final_candidates.insert(len(final_candidates), each)\n",
    "\n",
    "        final_candidates.insert(len(final_candidates), tfscore[0][0])\n",
    "        final_candidates.insert(len(final_candidates), tfscore[1][0])\n",
    "\n",
    "        t3_per = second_candidates[0:3]\n",
    "        for each in t3_per:\n",
    "            if each not in final_candidates:\n",
    "                final_candidates.insert(len(final_candidates), each)\n",
    "        \n",
    "        othertops = [num_score[0][0], per_score[0][0], tfscore[0][0], order_score[0][0]]\n",
    "        for top in othertops:\n",
    "            if top not in final_candidates:\n",
    "                final_candidates.insert(len(final_candidates), top)\n",
    "        \n",
    "    except:\n",
    "        othertops = [num_score[0][0], num_score[1][0], num_score[2][0], per_score[0][0], tfscore[0][0]]\n",
    "        for top in othertops:\n",
    "            if top not in final_candidates:\n",
    "                final_candidates.insert(len(final_candidates), top)\n",
    "    \n",
    "    for index, results in enumerate(final_candidates):\n",
    "        if index < 5:\n",
    "            print(\"RESULT\", index + 1, \":\", alldocslist[results][0:500], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT 1 : INDONESIA SEES CPO PRICE RISING SHARPLY\n",
      "  Indonesia expects crude palm oil CPO\n",
      "  prices to rise sharply to between 450 and 550 dlrs a tonne FOB\n",
      "  sometime this year because of better European demand and a fall\n",
      "  in Malaysian output Hasrul Harahap junior minister for tree\n",
      "  crops told Indonesian reporters\n",
      "      Prices of Malaysian and Sumatran CPO are now around 332\n",
      "  dlrs a tonne CIF for delivery in Rotterdam traders said\n",
      "      Harahap said Indonesia would maintain its exports despite\n",
      "  making r ...\n",
      "RESULT 2 : INDONESIAN COMMODITY EXCHANGE MAY EXPAND\n",
      "  The Indonesian Commodity Exchange is\n",
      "  likely to start trading in at least one new commodity and\n",
      "  possibly two during calendar 1987 exchange chairman Paian\n",
      "  Nainggolan said\n",
      "      He told Reuters in a telephone interview that trading in\n",
      "  palm oil sawn timber pepper or tobacco was being considered\n",
      "      Trading in either crude palm oil CPO or refined palm oil\n",
      "  may also be introduced But he said the question was still\n",
      "  being considered by Trade Minist ...\n",
      "RESULT 3 : MALAYSIA MAY NOT MEET 1987 OIL PALM TARGET\n",
      "  Malaysia is unlikely to meet its\n",
      "  targeted output of five mln tonnes of oil palm in calendar\n",
      "  1987 oil palm growers told Reuters\n",
      "      Output in 1987 is expected to reach around 45 mln tonnes\n",
      "  unchanged from 1986 because of drought low use of fertiliser\n",
      "  and overstressed palms they said\n",
      "      The growers were asked for their reaction to an Oil World\n",
      "  newsletter report that Malaysias oil palm output is likely to\n",
      "  drop sharply this year\n",
      "      Palm ...\n",
      "RESULT 4 : CRUDE OIL NETBACKS UP SHARPLY IN EUROPE US\n",
      "  Crude oil netback values in complex\n",
      "  refineries rose sharply in Europe and firmed in the US last\n",
      "  Friday from the previous week but fell sharply in Singapore\n",
      "  according to calculations by Reuters Pipeline\n",
      "      The firmer tone to refining margins in Europe and the US\n",
      "  relected higher prices for petroleum products particularly\n",
      "  gasoline and support from crude oil prices\n",
      "      Netback values for crude oil refined in Northern Europe\n",
      "  rose substanti ...\n",
      "RESULT 5 : SAUDI ARABIA SEEKING RBD PALM OLEIN\n",
      "  Saudi Arabia is in the market for 4000\n",
      "  tonnes of refined bleached deodorised palm olein for June 110\n",
      "  shipment traders said\n",
      "  \n",
      "\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "rank('indonesia crude palm oil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT 1 : CHINA CHILE TO BUILD COPPER TUBE PLANT IN CHINA\n",
      "  Chinas stateowned Beijing NonFerrous\n",
      "  Metals Industrial Corp and ltWrought Copper Ltd of Chile signed\n",
      "  a contract to jointly build a copper tube plant on the\n",
      "  outskirts of Peking the China Daily said\n",
      "      The BeijingSantiago Copper Tube Co involves an investment\n",
      "  of 993 mln dlrs and will on completion have a production\n",
      "  capacity of 5000 tonnes of copper tubes a year it said\n",
      "      It said Chile will supply copper at preferential rates to\n",
      "  t ...\n",
      "RESULT 2 : NIPPON STEEL DENIES CHINA SEEKING JAPANESE PLANTS\n",
      "  Nippon Steel Corp ltNSTCT denied local\n",
      "  newspaper reports that China has been seeking to buy steel\n",
      "  plants from Japanese firms which plan to suspend output under\n",
      "  the recently announced rationalisation program\n",
      "      The Mainichi Shimbun quoted Nippon Steel as saying that\n",
      "  Chinas State Planning Commission and some Chinese firms have\n",
      "  asked Japanese makers to sell them steel works and rolling\n",
      "  mills to expand steelmaking cheaply It named no ...\n",
      "RESULT 3 : CHINA RAISES GRAIN PURCHASE PRICES\n",
      "  China has raised the state purchase\n",
      "  prices of corn rice cottonseed and shelled peanuts from April\n",
      "  1 to encourage farmers to grow them the official China\n",
      "  Commercial Daily said\n",
      "      The paper said the price paid for corn from 14 northern\n",
      "  provinces cities and regions has increased by one yuan per 50\n",
      "  kg A foreign agricultural expert said the rise will take the\n",
      "  price to 17 fen per jin 05 kg from 16 fen\n",
      "      The paper said the price for longgrained ri ...\n",
      "RESULT 4 : CHINA DAILY SAYS VERMIN EAT 712 PCT GRAIN STOCKS\n",
      "  A survey of 19 provinces and seven cities\n",
      "  showed vermin consume between seven and 12 pct of Chinas grain\n",
      "  stocks the China Daily said\n",
      "      It also said that each year 1575 mln tonnes or 25 pct of\n",
      "  Chinas fruit output are left to rot and 21 mln tonnes or up\n",
      "  to 30 pct of its vegetables The paper blamed the waste on\n",
      "  inadequate storage and bad preservation methods\n",
      "      It said the government had launched a national programme to\n",
      "  reduce wa ...\n",
      "RESULT 5 : CHINA SULPHURIRON MINE STARTS PRODUCTION\n",
      "  Chinas largest sulphuriron mine has\n",
      "  started trial production at Yunfu in the southern province of\n",
      "  Guangdong the China Daily said\n",
      "      It said the mine has an annual output capacity of three mln\n",
      "  tonnes of sulphuriron ore which can be used without\n",
      "  processing because of its high quality\n",
      "  \n",
      "\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "rank('china')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
